{
    "contents" : "---\ntitle: \"Assignment 1\"\nauthor: \"Callin Switzer\"\ndate: \"September 8, 2014\"\noutput: html_document\n---\n\n```{r setup, results='hide', echo =FALSE}\n# function to load required packages\nipak <- function(pkg){\n  new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if(length(new.pkg)) install.packages(new.pkg, dependencies = TRUE)\n  sapply(pkg, require, character.only = TRUE)\n}\n\npackages <- c(\"ggplot2\", \"data.table\")\nipak(packages); rm(ipak); rm(packages)\n\n```\n\nnormal\n\n**bold**\n\n*italic*\n\n1. (Not Graded) Complete the following Conceptual Exercises in Chapter 1 (2nd or 3rd edition\nof R&S): 2, 7, 9. Feel free to include your answers in the write-up. However, this question will\nnot be graded.\n2. (20 points) For each of the following surveys, specify study units, the target population, and\nthe sampling frame. Discuss in 2-3 sentences any possible sources of selection bias, specifically,\nundercoverage or overcoverage. Finally, specify what type of sampling was used.\n     (a) (10 points) To estimate how many books in the library need rebinding, a librarian uses\n     a random number generator to select 100 locations on library shelves. He then walks to\n     each location, looks at the book that resides at that spot, and records whether the book\n     needs rebinding or not.\n          \n          <font style=\"background-color: yellow;\">\n               * Study units = books in the library <br>\n               * Target population = all the books in the library <br>\n               * Sampling frame = all the books in the library <br>\n               * Selection Bias: Undercoverage could occur if books were checked out or misplaced\n               , and the librarian was unable to find them on the shelves.  Overcoverage could occur\n               if the librarian found books that were not supposed to be in the libary -- for \n               example, someone could have returned a book to the library that didn't belong \n               to the library. <br>\n               * Sampling = simple random sample\n          \n          </font>\n\n\n     (b) (10 points) The Arizona Intrastate Travel Committee commissioned a study to identify\n     in-state travel patterns of residents of major metropolitan cities and to evaluate different\n     sources of vacation planning information. The plan was to conduct phone interviews with\n     Phoenix and Tucson residents. Landline telephone numbers with Phoenix and Tucson\n     area codes were generated randomly so that listed and unlisted telephone numbers could\n     be reached. (Arizona Office of Tourism, 1991.)\n          \n          <font style=\"background-color: yellow;\">\n               * Study units = residents of major Phoenix and Tucson with phone numbers in those area codes <br>\n               * Target population = all the road-users in the major metropolitan cities <br>\n               * Sampling frame = all the people with a landline in Phoenix and Tucson <br>\n               * Selection bias: Undercoverage could occur if people lived in Phoenix or Tucson \n               and didn't have a landline phone, or didn't answer their phone.  Overcoverage could\n               occur if someone who wasn't a resident of a major metropolitan area answered the phone -- \n               for example, someone could have holiday home in a major city, and not live there for most \n               of the year. <br>\n               * Sampling = simple random sample\n          </font>\n\n3. (5 points) Consider two securities, the first having expected return µ1 = 1 and standard\ndeviation σ1 = 0.1, and the second having µ2 = 0.8 and σ = 0.12. Also, let the correlation\nbetween securities be ρ = 0.1. Suppose you invest π = 0.8, or 80%, of your money in the first\nsecurity and 1-π in the second security. What is your expected return and what is the standard\ndeviation of your return?\n\n```{r, results = 'hide', echo = F}\nmo <- 0.8*(1) + 0.2*(0.8)\nva <- sqrt(0.8) * 0.1^2 + sqrt(0.2) * 0.12^2 + 2 * 0.8 * 0.2 * 0.1 * sqrt(0.1 + 0.12)\n```\n\n     <font style=\"background-color: yellow;\">\n          E(X)      = π(X1) + (1-π)(X2) <br>\n                    = 0.8(1) + 0.2(0.8) <br> \n                    = `r I(mo)` is the expected return <br> </font>\n     \n     <font style=\"background-color: yellow;\">         \n     Var(X)    = sqrt(π) * Var(X1) + sqrt(1-π) * Var(X2) + 2 * π * (1-π) Cov(X1,X2) <br>\n               = sqrt(0.8) * 0.1^2 + sqrt(0.2) * 0.12^2 + 2 * 0.8 * 0.2 * 0.1 * sqrt(0.1 + 0.12) <br>\n               = `r va` is the standard deviation of the return<br>   \n     </font>\n\n4.  (10 points) Solve the following equations for the matrix X. You can assume that the matrices\nA and B (when needed) are all invertible n x n matrices.\n(a) AXB = C\n(b) (AX) + B = D\n(c) Solve equation (b) for X by hand if\n\n\n\n\n\n\n\n\n\n5.  (15 points) The csdata.txt data set on the course web-site contains information on 224\ncomputer science students. Use R to perform the following tasks:\n     (a) Split the students into two groups with GPA < 3 and GPA ≥ 3 and provide the following\n     numerical summaries of the distributions of SAT Math (SATM) scores for each of the two\n     groups: sample mean, sample SD, min, median, max, 1st and 3rd quartiles.\n```{r split and summarize, results='hold'}\n# import data\nstud <- read.table(\"data/CSDATA.txt\", header = T)\nstud$gpaGroup <- ifelse(stud$GPA > 3, yes = \"High\", no = \"Low\")\n\n# split into low and high\nhigh <- stud[stud$gpaGroup == \"High\", ]\nlow <- stud[ stud$gpaGroup == \"Low\",]\n\n# custom summary function to include sd\nmysum <- function(vector) {\n     s <- numeric(7)\n     names(s) <- c(\"sample mean\", \"sample SD\", \"min\", \"median\", \n                   \"max\", \"1st quartile\",\"3rd quartile\") \n     s[c(1,3,4,5,6,7)] <- summary(vector)[c(4, 1, 3, 6, 2, 5)]\n     s[2] <- round(sd(vector), digits = 1)\n     return(s)\n}\n```\n<font style=\"background-color: yellow;\"> \nSummary for high-gpa students\n```{r, echo = F}\nmysum(high$SATM)\n```\n\n<font style=\"background-color: yellow;\"> Summary for low-gpa students\n```{r, echo = F}\nmysum(low$SATM)\n```\n\n(b) Plot histograms and box-plots of SATM scores for both groups side-by-side and describe\nthe shapes of their distributions. Are there any visible differences?\n\n```{r hist, results = \"hold\", echo = F, fig.width= 6, fig.height=4}\nstud$gpaGroup <- factor(stud$gpaGroup)\n\nggplot(stud) + \n     geom_histogram(aes(SATM, fill = gpaGroup), binwidth = 50, color = \"grey40\") +\n     theme_bw() +\n     facet_wrap(~gpaGroup) + \n     theme(legend.position = \"none\") +  \n     ggtitle(\"SAT Math scores for high and low gpa students\") + \n     labs(x = \"SAT Math Scores\")\n\n\nggplot(stud, aes(x = gpaGroup, y = SATM)) +\n     geom_boxplot(aes(fill = gpaGroup), color = \"grey40\") + \n     theme_bw() +\n     theme(legend.position = \"none\") + \n     ggtitle(\"Boxplot of SAT Math scores for low and high gpa students\")+ \n     labs(x = \"GPA Group\", y =\"SAT Maths Score\")\n```\n\n<font style=\"background-color: yellow;\"> Both distributions seem to be fairly normally distributed. They both have similar variances and neither seems especially skewed.\nThere are more poeple in the low-gpa group, as evidenced by the taller histogram.  The distribution\nof SATM scores from the high-gpa group has a slightly higher mean than the low-gpa group. </font>\n\n(c) Comment on whether you think the group means are very different or not (without con- ducting any formal tests).\n<font style=\"background-color: yellow;\"> The sample group means differ by about 40 points. This is quite a large difference -- I suspect that it's highly unlikely to find that large of a difference if the sample was random.  Thus, I think the group means are different. </font>\n\n(d) Calculate the overall median SATM score in the sample and interpret it.\n```{r median, echo = FALSE}\nmm <- median(stud$SATM)\n```\n<font style=\"background-color: yellow;\">\nThe overall median SATM score is `r mm`, which is basically right in the middle of two groups. Since the data are distributed fairly normally, the median is quite close to the mean.  The median is the score that 50% of the other scores fall below, and 50% fall above. </font>\n\n\n\n(e) SAT is calibrated such that scores in the entire student population are distributed approx- imately normally with mean 500 and standard deviation 110. Identifying what fraction of student population is expected to score above the sample median found in the previous part? (Hint: use the R command pnorm() to find normal probabilities.) Are your findings consistent with an assertion that CS students have stronger math skills?",
    "created" : 1410213984203.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2541145845",
    "id" : "87644B95",
    "lastKnownWriteTime" : 1410231330,
    "path" : "~/Documents/S139/assignment1.rmd",
    "project_path" : "assignment1.rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}